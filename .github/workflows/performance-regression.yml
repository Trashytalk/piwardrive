name: Performance Regression Testing

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'requirements*.txt'
      - 'package*.json'
      - 'benchmarks/**'
      - '.github/workflows/performance-regression.yml'
  push:
    branches: [ main ]
  schedule:
    # Run performance tests weekly on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      baseline_ref:
        description: 'Baseline reference (branch/tag/commit)'
        required: false
        default: 'main'
      comparison_ref:
        description: 'Comparison reference (branch/tag/commit)'
        required: false
        default: 'HEAD'
      test_duration:
        description: 'Test duration in minutes'
        required: false
        default: '10'
        type: choice
        options:
          - '5'
          - '10'
          - '30'
          - '60'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'
  PERFORMANCE_THRESHOLD: '10'  # 10% performance degradation threshold

jobs:
  performance-baseline:
    name: Establish Performance Baseline
    runs-on: ubuntu-latest
    
    outputs:
      baseline-results: ${{ steps.baseline.outputs.results }}
      
    steps:
      - name: Checkout baseline code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.baseline_ref || 'main' }}
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r config/requirements.txt
          pip install -r config/requirements-dev.txt
          pip install pytest-benchmark pytest-xdist
          
      - name: Set up test environment
        run: |
          # Create test database
          python scripts/setup_test_env.py --type performance
          
          # Generate test data
          python scripts/generate_test_data.py --size large
          
      - name: Run baseline performance tests
        id: baseline
        run: |
          echo "Running baseline performance tests..."
          
          # Run performance benchmarks
          python -m pytest benchmarks/ -v \
            --benchmark-json=baseline-performance.json \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-sort=mean \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=3
            
          # Run load tests
          python -m pytest tests/test_performance_comprehensive.py -v \
            --benchmark-json=baseline-load-test.json \
            --test-duration=${{ github.event.inputs.test_duration || '10' }}
            
          # Run memory profiling
          python scripts/memory_profiling.py --output baseline-memory-profile.json
          
          # Run CPU profiling
          python scripts/cpu_profiling.py --output baseline-cpu-profile.json
          
          echo "results=baseline-performance.json" >> $GITHUB_OUTPUT
          
      - name: Upload baseline results
        uses: actions/upload-artifact@v4
        with:
          name: baseline-performance-results
          path: |
            baseline-performance.json
            baseline-load-test.json
            baseline-memory-profile.json
            baseline-cpu-profile.json
            
  performance-comparison:
    name: Performance Comparison Testing
    runs-on: ubuntu-latest
    needs: [performance-baseline]
    
    steps:
      - name: Checkout comparison code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.comparison_ref || 'HEAD' }}
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r config/requirements.txt
          pip install -r config/requirements-dev.txt
          pip install pytest-benchmark pytest-xdist
          
      - name: Set up test environment
        run: |
          # Create test database
          python scripts/setup_test_env.py --type performance
          
          # Generate test data
          python scripts/generate_test_data.py --size large
          
      - name: Run comparison performance tests
        run: |
          echo "Running comparison performance tests..."
          
          # Run performance benchmarks
          python -m pytest benchmarks/ -v \
            --benchmark-json=comparison-performance.json \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-sort=mean \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=3
            
          # Run load tests
          python -m pytest tests/test_performance_comprehensive.py -v \
            --benchmark-json=comparison-load-test.json \
            --test-duration=${{ github.event.inputs.test_duration || '10' }}
            
          # Run memory profiling
          python scripts/memory_profiling.py --output comparison-memory-profile.json
          
          # Run CPU profiling
          python scripts/cpu_profiling.py --output comparison-cpu-profile.json
          
      - name: Download baseline results
        uses: actions/download-artifact@v5
        with:
          name: baseline-performance-results
          
      - name: Compare performance results
        run: |
          echo "Comparing performance results..."
          
          python scripts/compare_performance.py \
            --baseline baseline-performance.json \
            --comparison comparison-performance.json \
            --output performance-comparison.json \
            --threshold ${{ env.PERFORMANCE_THRESHOLD }} \
            --format json
            
          python scripts/compare_performance.py \
            --baseline baseline-performance.json \
            --comparison comparison-performance.json \
            --output performance-comparison.md \
            --threshold ${{ env.PERFORMANCE_THRESHOLD }} \
            --format markdown
            
      - name: Upload comparison results
        uses: actions/upload-artifact@v4
        with:
          name: performance-comparison-results
          path: |
            comparison-performance.json
            comparison-load-test.json
            comparison-memory-profile.json
            comparison-cpu-profile.json
            performance-comparison.json
            performance-comparison.md
            
      - name: Check for performance regressions
        id: regression-check
        run: |
          echo "Checking for performance regressions..."
          
          # Check if any regressions were detected
          if python -c "
          import json
          with open('performance-comparison.json') as f:
              data = json.load(f)
          exit(1 if data.get('regressions_detected', False) else 0)
          "; then
            echo "regression-detected=false" >> $GITHUB_OUTPUT
          else
            echo "regression-detected=true" >> $GITHUB_OUTPUT
          fi
          
      - name: Comment on PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('performance-comparison.md')) {
              const performanceReport = fs.readFileSync('performance-comparison.md', 'utf8');
              
              const comment = `## üìä Performance Regression Test Results
              
              ${performanceReport}
              
              <details>
              <summary>üìà Detailed Performance Metrics</summary>
              
              - **Baseline**: ${{ github.event.inputs.baseline_ref || 'main' }}
              - **Comparison**: ${{ github.event.inputs.comparison_ref || 'HEAD' }}
              - **Test Duration**: ${{ github.event.inputs.test_duration || '10' }} minutes
              - **Threshold**: ${{ env.PERFORMANCE_THRESHOLD }}%
              
              </details>`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
            
      - name: Fail if performance regression detected
        if: steps.regression-check.outputs.regression-detected == 'true'
        run: |
          echo "‚ùå Performance regression detected!"
          echo "Please review the performance comparison results."
          exit 1
          
  stress-testing:
    name: Stress Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    strategy:
      matrix:
        test-type: [
          'high-load',
          'memory-stress',
          'cpu-stress',
          'io-stress',
          'network-stress'
        ]
        
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r config/requirements.txt
          pip install -r config/requirements-dev.txt
          pip install locust psutil
          
      - name: Set up stress test environment
        run: |
          # Create stress test database
          python scripts/setup_test_env.py --type stress
          
          # Generate large test dataset
          python scripts/generate_test_data.py --size stress
          
      - name: Run stress tests
        run: |
          echo "Running ${{ matrix.test-type }} stress tests..."
          
          case "${{ matrix.test-type }}" in
            "high-load")
              python scripts/stress_test_high_load.py --duration 30 --output stress-high-load.json
              ;;
            "memory-stress")
              python scripts/stress_test_memory.py --duration 30 --output stress-memory.json
              ;;
            "cpu-stress")
              python scripts/stress_test_cpu.py --duration 30 --output stress-cpu.json
              ;;
            "io-stress")
              python scripts/stress_test_io.py --duration 30 --output stress-io.json
              ;;
            "network-stress")
              python scripts/stress_test_network.py --duration 30 --output stress-network.json
              ;;
          esac
          
      - name: Upload stress test results
        uses: actions/upload-artifact@v4
        with:
          name: stress-test-results-${{ matrix.test-type }}
          path: stress-*.json
          
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: piwardrive_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r config/requirements.txt
          pip install -r config/requirements-dev.txt
          pip install locust
          
      - name: Set up load test environment
        run: |
          # Set up database
          PGPASSWORD=postgres psql -h localhost -U postgres -d piwardrive_test -f scripts/setup_test_db.sql
          
          # Start application
          python -m src.main &
          sleep 10  # Wait for startup
          
      - name: Run load tests
        run: |
          echo "Running load tests..."
          
          # Run Locust load tests
          locust -f tests/locustfile.py --headless \
            --users 100 \
            --spawn-rate 10 \
            --run-time 300s \
            --host http://localhost:8080 \
            --html load-test-report.html \
            --csv load-test-results
            
      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test-results*.csv
            
  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r config/requirements.txt
          pip install -r config/requirements-dev.txt
          pip install memory-profiler pympler objgraph
          
      - name: Run memory profiling
        run: |
          echo "Running memory profiling..."
          
          # Memory usage profiling
          python scripts/memory_profiling.py --detailed --output memory-profile.json
          
          # Memory leak detection
          python scripts/memory_leak_detection.py --output memory-leak-report.json
          
          # Object growth analysis
          python scripts/object_growth_analysis.py --output object-growth.json
          
      - name: Upload memory profiling results
        uses: actions/upload-artifact@v4
        with:
          name: memory-profiling-results
          path: |
            memory-profile.json
            memory-leak-report.json
            object-growth.json
            
  cpu-profiling:
    name: CPU Profiling
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r config/requirements.txt
          pip install -r config/requirements-dev.txt
          pip install cProfile pstats py-spy
          
      - name: Run CPU profiling
        run: |
          echo "Running CPU profiling..."
          
          # CPU usage profiling
          python scripts/cpu_profiling.py --output cpu-profile.json
          
          # Performance hotspots analysis
          python scripts/performance_hotspots.py --output hotspots-analysis.json
          
          # Function call analysis
          python scripts/function_call_analysis.py --output function-calls.json
          
      - name: Upload CPU profiling results
        uses: actions/upload-artifact@v4
        with:
          name: cpu-profiling-results
          path: |
            cpu-profile.json
            hotspots-analysis.json
            function-calls.json
            
  performance-report:
    name: Generate Performance Report
    runs-on: ubuntu-latest
    if: always()
    needs: [
      performance-baseline,
      performance-comparison,
      stress-testing,
      load-testing,
      memory-profiling,
      cpu-profiling
    ]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download all artifacts
        uses: actions/download-artifact@v5
        
      - name: Generate comprehensive performance report
        run: |
          echo "Generating comprehensive performance report..."
          
          python scripts/generate_performance_report.py \
            --baseline-results baseline-performance-results/ \
            --comparison-results performance-comparison-results/ \
            --stress-results stress-test-results-*/ \
            --load-results load-test-results/ \
            --memory-results memory-profiling-results/ \
            --cpu-results cpu-profiling-results/ \
            --output performance-report.html \
            --format html
            
          python scripts/generate_performance_report.py \
            --baseline-results baseline-performance-results/ \
            --comparison-results performance-comparison-results/ \
            --stress-results stress-test-results-*/ \
            --load-results load-test-results/ \
            --memory-results memory-profiling-results/ \
            --cpu-results cpu-profiling-results/ \
            --output performance-report.json \
            --format json
            
      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: |
            performance-report.html
            performance-report.json
            
      - name: Archive performance data
        run: |
          # Archive performance data for historical tracking
          timestamp=$(date +%Y%m%d_%H%M%S)
          mkdir -p performance-archive/$timestamp
          
          cp performance-report.json performance-archive/$timestamp/
          cp -r baseline-performance-results performance-archive/$timestamp/
          cp -r performance-comparison-results performance-archive/$timestamp/
          
          # Upload to performance database
          python scripts/upload_performance_data.py \
            --report performance-report.json \
            --timestamp $timestamp
            
      - name: Send performance summary
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('performance-report.json')) {
              const report = JSON.parse(fs.readFileSync('performance-report.json', 'utf8'));
              
              const summary = `## üìä Performance Test Summary
              
              **Overall Status**: ${report.overall_status}
              **Regressions Detected**: ${report.regressions_detected ? 'Yes' : 'No'}
              **Performance Score**: ${report.performance_score}/100
              
              ### Key Metrics
              - **Throughput**: ${report.throughput.change}% change
              - **Latency**: ${report.latency.change}% change
              - **Memory Usage**: ${report.memory.change}% change
              - **CPU Usage**: ${report.cpu.change}% change
              
              ### Test Coverage
              - **Baseline Tests**: ${report.baseline_tests} passed
              - **Comparison Tests**: ${report.comparison_tests} passed
              - **Stress Tests**: ${report.stress_tests} passed
              - **Load Tests**: ${report.load_tests} passed
              `;
              
              if (context.payload.pull_request) {
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: summary
                });
              }
            }
